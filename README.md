# Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation

This repository contains the code for the paper "Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation". In this paper, we an innovative token-level manipulation approach to jailbreak open-source LLMs effectively and efficiently. Our paper is released on [Arxiv](https://arxiv.org/abs/2405.13068).


## Introduction

Large language models (LLMs) have transformed the field of natural language processing, but they remain susceptible to jailbreaking attacks that exploit their capabilities to generate unintended and potentially harmful content. Existing token-level jailbreaking techniques, while effective, face scalability and efficiency challenges, especially as models undergo frequent updates and incorporate advanced defensive measures. In this paper, we introduce JAILMINE, an innovative token-level manipulation approach that addresses these limitations effectively.

## Installation

To get started, clone this repository and install the required dependencies:

```bash
git clone https://github.com/yearsago162012/JailMine
cd JailMine
pip install -r requirements.txt
```
